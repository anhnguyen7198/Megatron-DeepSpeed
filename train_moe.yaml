$schema: https://componentsdk.azureedge.net/jsonschema/DistributedComponent.json
description: "FIM GPT Pretraining MoE"
display_name: FIM-GPT-Pretraining-MoE
environment:
  docker: 
    image: b5476ead59e14b909c1c0afdc3422078.azurecr.io/megatron-deepspeed-moe
  #   image: mcr.microsoft.com/azureml/curated/acpt-pytorch-1.11-py38-cuda11.5-gpu
  # conda:
  #   conda_dependencies:
  #     name: project_environment
  #     channels:
  #     - defaults
  #     dependencies:
  #     - python=3.8.13
  #     - pip:
  #       - wandb
  #       - deepspeed==0.8.0
  #       - regex
  # build:
  #   path: ./
  #   build.dockerfile_path:
  #     Dockerfile
  os: Linux
inputs: 
  train_dataset: 
    description: Training dataset
    optional: false
    type: path
  vocab_dataset: 
    description: Vocab dataset
    optional: false
    type: path

  EP_SIZE: 
    description: Number of experts
    optional: true
    default: 1
    type: int
  MODEL_SIZE: 
    description: model size
    optional: true
    default:  0.35
    type: float
  NUM_LAYERS: 
    description: num layers
    optional: true
    default: 24
    type: int
  NUM_ATTN_HEADS: 
    description: num attn hads
    optional: true
    default: 16
    type: int
  HIDDEN_SIZE:
    description: size of hidden layers
    optional: true
    default: 1024
    type: int
  GLOBAL_BATCH_SIZE:
    description:  affects training iterations
    optional: true
    default: 256
    type: int
  LR: 
    description: learning rate
    optional: true
    default: 2.0e-4
    type: float
  MIN_LR: 
    description: learning rate
    optional: true
    default: 2e-06
    type: float
  BATCH_SIZE:
    description:  Make sure that BATCH_SIZE <= GLOBAL_BATCH_SIZE*PP_SIZE*MP_SIZE/NUM_GPUS
    optional: true
    default: 4
    type: int
  NUM_GPUS: 
    description: number of gpus
    optional: true
    default: 64
    type: int
  MLC: 
    description: Coefficient for MoE loss. We find that 0.01 is a good value
    optional: true
    default: 0.01
    type: float
  EXIT_DURATION:
    description:  Exit cooldown for lr
    optional: true
    default: 30000000
    type: int
  TRAIN_TOKENS:
    description:  Total train tokens
    optional: true
    default: 300000000000
    type: int
  WARMUP_TOKENS:
    description:  LR warmup
    optional: true
    default: 375000000
    type: int
  LR_DECAY_TOKENS:
    description:  LR decay
    optional: true
    default: 300000000000
    type: int
  EVAL_INTERVAL:
    description:  interval between evaluating
    optional: true
    default: 100
    type: int
  SAVE_INTERVAL:
    description:  interval between saving
    optional: true
    default: 10000
    type: int


is_deterministic: true
meta: 
  requireGpu: true
name: pretraining.pretrain_moe
outputs: 
  model_checkpoint: 
    description: "Model checkpoints"
    optional: false
    type: path
  logging_path: 
    description: "Logging path"
    optional: false
    type: path
launcher:
  type: torch.distributed
  additional_arguments: >    
    ./pretrain_gpt_moe.sh 
      --data_path {inputs.train_dataset}
      --vocab_path {inputs.vocab_dataset}
      --output_path {outputs.model_checkpoint}
      --logging_path {outputs.logging_path}
      [--EP_SIZE {inputs.EP_SIZE}]
      [--MODEL_SIZE {inputs.MODEL_SIZE}]
      [--NUM_LAYERS {inputs.NUM_LAYERS}]
      [--HIDDEN_SIZE {inputs.HIDDEN_SIZE}]
      [--NUM_ATTN_HEADS {inputs.NUM_ATTN_HEADS}]
      [--GLOBAL_BATCH_SIZE {inputs.GLOBAL_BATCH_SIZE}]
      [--LR {inputs.LR}]
      [--MIN_LR {inputs.MIN_LR}]
      [--BATCH_SIZE {inputs.BATCH_SIZE}]
      [--NUM_GPUS {inputs.NUM_GPUS}]
      [--MLC {inputs.MLC}]
      [--EXIT_DURATION {inputs.EXIT_DURATION}]
      [--TRAIN_TOKENS {inputs.TRAIN_TOKENS}]
      [--WARMUP_TOKENS {inputs.WARMUP_TOKENS}]
      [--LR_DECAY_TOKENS {inputs.LR_DECAY_TOKENS}]
      [--EVAL_INTERVAL {inputs.EVAL_INTERVAL}]
      [--SAVE_INTERVAL {inputs.SAVE_INTERVAL}]

tags: 
  author: misantac@microsoft.com
type: DistributedComponent
version: 0.1.0